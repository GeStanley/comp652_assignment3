\documentclass{report}
% Change "article" to "report" to get rid of page number on title page
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{setspace}
\usepackage{Tabbing}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage{chngpage}
\usepackage{soul,color}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{graphicx,float,wrapfig}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{tikz}
\usepackage[]{algorithm2e}
% In case you need to adjust margins:
\topmargin=-0.45in      %
\evensidemargin=0in     %
\oddsidemargin=0in      %
\textwidth=6.5in        %
\textheight=9.0in       %
\headsep=0.25in         %

\title{Assignment 1 - Comp 652 - Machine Learning}

% Homework Specific Information
\newcommand{\hmwkTitle}{Assignment 3}                     % Adjust this
\newcommand{\hmwkDueDate}{Tuesday, March 31 2015}                           % Adjust this
\newcommand{\hmwkClass}{COMP 652}


\newcommand{\hmwkClassInstructor}{Dr. Doina Precup}
\newcommand{\hmwkAuthorName}{Geoffrey Stanley}
\newcommand{\hmwkAuthorNumber}{260645907}
\newcommand{\Pp}{\mathbb{P}}
\newcommand{\Ev}{\mathbb{E}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\dd}{\, \mathrm{d}}

% Setup the header and footer
\pagestyle{fancy}                                                       %
\lhead{\hmwkAuthorName}                              %
\chead{}
\rhead{\hmwkClass: \hmwkTitle}                                          %

\lfoot{}
\cfoot{}                                                                %
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}}                          %
\renewcommand\headrulewidth{0.4pt}                                      %
\renewcommand\footrulewidth{0.4pt}                                      %

% This is used to trace down (pin point) problems
% in latexing a document:
%\tracingall
\definecolor{mygreen}{rgb}{0,0.6,0}
\lstset{commentstyle=\color{mygreen}, frame=single,  language=R, showspaces=false, showstringspaces=false}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make title
\title{\vspace{2in}\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{Presented to \hmwkClassInstructor}}\vspace{3in}}
\date{}
\author{\textbf{\hmwkAuthorName}\\
    \textbf{Student ID: \hmwkAuthorNumber}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle
\section*{Question 1}
\subsection*{A)}
In spin glass models the maximum likelihood is computed as the sum of the products
of the maximal clique's energy for each node in the model.
\begin{equation}
  \log L(\psi | D) = \sum_{i=1}^N \log \frac{1}{Z} \prod_C \psi_C(x_C)
\end{equation}
In a 4-neighbor spin glass model the maximal cliques were the edges between
each pixel. As illustrated below:
\begin{center}
  \includegraphics[width=125pt, keepaspectratio=true]{ising_model_4n.jpg}\\
\end{center}
\begin{equation}
  P(E) = \psi (x, Y = \{B,E\}) \psi (x, Y = \{D,E\}) \psi (x, Y = \{E,F\}) \psi (x, Y = \{E,H\})
\end{equation}
In an 8-neighbor spin glass model the maximal cliques become
clusters of 4 nodes. As such, the energy of a node will be computed as the
product of 4 clusters of 4:
\begin{center}
  \includegraphics[width=125pt, keepaspectratio=true]{ising_model_8n.jpg}\\
\end{center}
\begin{equation}
  P(E) = \psi (x, Y = \{A,B,D,E\}) \psi (x, Y = \{B,C,E,F\}) \psi (x, Y = \{D,E,G,H\}) \psi (x, Y = \{E,F,H,I\})
\end{equation}
The energy computation can remain the same however.
\begin{equation}
  \psi(x, Y) = \alpha x + \sum_{i} \beta_{i} y_i x
\end{equation}
Where $i$ ranges from $0$ to $7$ for the 8 neighbors of a particular node.
\subsection*{B)}
The advantages and disadvantages would be related to a trade off between
model precision and computation time.\\

Given a 4 neighbor model computation time will be smaller given 20 fewer edge
energies to be computed. However, this is at the expense of model accuracy. With
the 8 neighbor model more data will be used to infer the value of a pixel as well
as being more flixible due to using information coming from 4 new angles. \\

This should improve the models accuracy. In particular, when working with an image
that contains circular shapes or curved lines.
\subsection*{C)}
A gibbs sampling algorithm in this situation, where evidence is injected along
the left most edge, would require two passes. The first, would be a forward pass
using only two of the four neighbor pixels. In the case of the first column, it
will use the injected pixel to the left as well as the last infered pixel. Subsequent
columns would use the two previous infered pixels.\\

The second would be a backward pass and would use all four pixels. It would start
from the bottom and works it's way back to the initially infered pixel. This would
ensure that all pixels were estimated given the probability distribution of it's four
neighbors.


\section*{Question 2}
\begin{center}
\includegraphics[width=250pt, keepaspectratio=true]{reconstruction_error.jpg}\\
\end{center}
As dimensions are reduced from 250 the reconstruction error is initially quite
small but becomes more important as dimensions approach 0. The
shoulder of the reconstruction error line is at a dimension of approximately 25.\\

From this, we can conclude that given this input space we can use PCA to
reduce dimensionality of the data to 25 in order to make computation more manageable
without losing very much granularity in the feature data.
\section*{Question 3}
\subsection*{A)}
As with standard Hidden Markov Models, Coupled Hidden Markov models will
have three categories of parameters. These are the initial probabilities, the
transition probabilites and the emission probabilities. Given the system
depicted in Figure 1 of assignment 3:\\

Initial Probabilities:
\begin{equation}
  P(s0)
\end{equation}
\begin{equation}
  P(u0)
\end{equation}\\

Transition Probabilities:
\begin{equation}
  P(s_i | s_{i-1}, u_{i-1})
\end{equation}
\begin{equation}
  P(u_i | u_{i-1}, s_{i-1})
\end{equation}\\

Emission Probabilities:
\begin{equation}
  P(y_i | s_i)
\end{equation}
\begin{equation}
  P(z_i | u_i)
\end{equation}

\subsection*{B)}
In order to compute the joint probability of a sequence of observations a
forward algorithm will need to be derived.

\begin{equation}
  \begin{aligned}
  \alpha_t(s_t, u_t) & = P(s_t, u_t, y_{0:T}, x_{0:T})\\
   & = \sum_{t = 1}^T p(s_t, s_{t-1}, u_t, u_{t-1}, y_{1:T}, x_{1:T})\\
   & = \sum_{t = 1}^T p(y_t | s_t) p(x_t | u_t) p(s_t | s_{t-1}, u_{t-1}) p(u_t | s_{t-1}, u_{t-1}) p(s_{t-1}, u_{t-1}, y_{1:T-1}, x_{1:T-1})\\
   & = \sum_{t = 1}^T p(y_t | s_t) p(x_t | u_t) p(s_t | s_{t-1}, u_{t-1}) p(u_t | s_{t-1}, u_{t-1}) \alpha_{t-1}(y_{t-1}, x_{t-1})
  \end{aligned}
\end{equation}
Because the equation listed above does not contain the initial probability we
need to compute it seperately:
\begin{equation}
  \alpha_0 (s_0, u_0) = p(y_0, z_0, s_0, u_0) = p(s_0) p(y_0 | s_0) p(u_0) p(z_0 | u_0)
\end{equation}
Summing the equations above we obtain the joint probability:
\begin{equation}
  p(y_0, z_0, y_1, z_1, ... , y_T, z_T) = \sum_{i=0}^T \alpha_i(s_i, u_i)
\end{equation}

\subsection*{C)}
The forward-backward algorithm is equal to the product between the forward and the
backward algorithm. In the previous question I derived the forward algorithm. So,
what is remaining is to derive the backward algorithm:
\begin{equation}
  \begin{aligned}
    \beta_t(s_t, u_t) & = p(y_{t+1:n} | s_t, u_t) p(z_{t+1:n} | s_t, u_t)\\
    & = \sum_{k=0}^{T-1} p(y_{t+1:n}, z_{t+1:n}, s_{t+1}, u_{t+1} | s_t, u_t)\\
    & = \sum_{k=0}^{T-1} p(y_{t+2:n} | s_{t+1}, u_{t+1}) p(z_{t+2:n} | s_{t+1}, u_{t+1}) p(y_{t+1} | s_{t+1}) p(z_{t+1} | u_{t+1}) p(s_{t+1} | s_t, u_t) p(u_{t+1} | s_t, u_t)\\
    & = \sum_{k=0}^{T-1} \beta_{t+1}(s_{t+1}, u_{t+1}) p(y_{t+1} | s_{t+1}) p(z_{t+1} | u_{t+1}) p(s_{t+1} | s_t, u_t) p(u_{t+1} | s_t, u_t)
  \end{aligned}
\end{equation}
The probability at time T will be equal to 1 :
\begin{equation}
  \beta_T (s_T, u_T) = 1
\end{equation}
Summing the equations above we obtain te backward algorithm:
\begin{equation}
p(y_{t+1:n} | s_t, u_t) p(z_{t+1:n} | s_t, u_t) = \sum_{i=0}^{T} \beta_i(s_i, u_i)
\end{equation}
We can now derive the forward-backward algorithm as the product between the forward
and the backward algorithm:
\begin{equation}
p(s_t, u_t | y_{0:T}, x_{0:T}) = \left(\sum_{i=0}^T \alpha_i(s_i, u_i)\right)\left( \sum_{i=0}^{T} \beta_i(s_i, u_i)\right)
\end{equation}

\subsection*{D)}
The transition probabilities as well as the inference algorithms used will be
conditional on whether $t\mod{k}=0$. More precisely, for transition probabilities:

\begin{equation}
  \left\{
  \begin{array}{l l}
    P(s_i | s_{i-1}, u_{i-1}) & \quad \text{if } i \mod k = 0 \\
    P(s_i | s_{i-1}) & \quad \text{otherwise}

  \end{array} \right.
\end{equation}

\begin{equation}
  \left\{
  \begin{array}{l l}
    P(u_i | u_{i-1}, s_{i-1}) & \quad \text{if } i \mod k = 0 \\
    P(u_i | u_{i-1}) & \quad \text{otherwise}

  \end{array} \right.
\end{equation}\\
And the computation for the inference algorithms will be similar. For the forward
algorithm:
\begin{equation}
  \left\{
  \begin{array}{l l}
  \sum_{t = 1}^T p(y_t | s_t) p(x_t | u_t) p(s_t | s_{t-1}, u_{t-1}) p(u_t | s_{t-1}, u_{t-1}) \alpha_{t-1}(y_{t-1}, x_{t-1}) & \quad \text{if } t \mod k = 0 \\
  \sum_{t = 1}^T p(y_t | s_t) p(x_t | u_t) p(s_t | s_{t-1}) p(u_t | u_{t-1}) \alpha_{t-1}(y_{t-1}, x_{t-1}) & \quad \text{otherwise}

  \end{array} \right.
\end{equation}\\
And for the backward algorithm:
\begin{equation}
  \left\{
  \begin{array}{l l}
  \sum_{t=0}^{T-1} \beta_{t+1}(s_{t+1}, u_{t+1}) p(y_{t+1} | s_{t+1}) p(z_{t+1} | u_{t+1}) p(s_{t+1} | s_t, u_t) p(u_{t+1} | s_t, u_t) & \quad \text{if } i \mod k = 0 \\
  \sum_{t=0}^{T-1} \beta_{t+1}(s_{t+1}, u_{t+1}) p(y_{t+1} | s_{t+1}) p(z_{t+1} | u_{t+1}) p(s_{t+1} | s_t) p(u_{t+1} | u_t) & \quad \text{otherwise}

  \end{array} \right.
\end{equation}\\
\subsection*{E)}
The object of the learning algorithm should be to find the $K$ whose joint probability
is the greatest. As such, for each $K$ in the the range $0$ to $n$ the joint probability
will have to be calculated. Once this is done, the $K$ whose joint probability is the
greatest should be used as the most accurate model.

\end{document}
